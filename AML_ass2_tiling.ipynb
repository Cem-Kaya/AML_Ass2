{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Advanced Machine Learning - programming assignment 2\n",
    "\n",
    "*Due: Monday December 16th* (**NOON**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in:**\n",
    "* Cem Kaya (9276866)\n",
    "* Gaynora van Dommelen (6717659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further instructions:\n",
    "* Code quality is considered during the assessement. Make sure your code is properly commented.\n",
    "* You can find the required python packages in requirements.txt (Keep in mind, the grader most likely won't install additional packages. Try to stick with the standard library and the packages listed). Also, we recommend to use python 3.10. \n",
    "* Submit your code in Blackboard using one of your accounts; we will put the grade in Blackboard for the other team member as well.\n",
    "* **Make sure to name the submitted file according to your and your collaborators last name.** (submitter_collaborator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Reinforcement learning with function approximation\n",
    "\n",
    "In this assignment, you'll design your own agent to complete an episodic MDP task following the gymnasium (gym) framework. The agent will be looking at a small part of the UU logo, and have to decide which of the four compass directions (i.e. left, right, up, down) to move in. The learning task is to find the goal in the center as soon as possible.\n",
    "\n",
    "The learning objectives of this assignment are:\n",
    "\n",
    "- Implement two versions of the agent using Semi-gradient SARSA and Q-learning algorithms with a linear approximation function,\n",
    "- Demonstrate the difference between on-policy and off-policy RL methods,\n",
    "- Learn to integrate the approximation function with Tabular RL methods,\n",
    "- Play with the parameters discount factor $\\gamma$ and stepsize $\\alpha$ and understand their influence on the learning procedure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Let's start with setting up the enviroment.\n",
    "\n",
    "The following code defines various aspects of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import IntEnum\n",
    "from typing import List, Tuple, Dict, Union, NamedTuple\n",
    "\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib.figure import Figure\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm  # For progress bars\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Constants defining the environment\n",
    "GOAL = (140, 120)\n",
    "CENTER = (132, 132)\n",
    "AVG_MOVEMENT_SIZE = 24\n",
    "ACCEPTABLE_DISTANCE_TO_GOAL = (AVG_MOVEMENT_SIZE // 2) + 1\n",
    "RADIUS = 72\n",
    "WINDOW_SIZE = 28\n",
    "TIME_LIMIT = 200\n",
    "TIMEOUT_REWARD = -100.0\n",
    "MOVE_REWARD = -1\n",
    "INVALID_MOVE_REWARD = -5\n",
    "\n",
    "\n",
    "# This is for type inference\n",
    "State = Tuple[int, int]\n",
    "\n",
    "\n",
    "# Action space\n",
    "class Actions(IntEnum):\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "\n",
    "# Boundaries\n",
    "class Boundary(IntEnum):\n",
    "    WEST = CENTER[0] - RADIUS\n",
    "    EAST = CENTER[0] + RADIUS\n",
    "    NORTH = CENTER[1] - RADIUS\n",
    "    SOUTH = CENTER[1] + RADIUS\n",
    "\n",
    "# Augmented boundarie, not used in this assignment\n",
    "class AugmentedArea(IntEnum):\n",
    "    WEST = Boundary.WEST - (WINDOW_SIZE // 2)\n",
    "    EAST = Boundary.EAST + (WINDOW_SIZE // 2)\n",
    "    NORTH = Boundary.NORTH - (WINDOW_SIZE // 2)\n",
    "    SOUTH = Boundary.SOUTH + (WINDOW_SIZE // 2)\n",
    "\n",
    "\n",
    "# Image\n",
    "ORIGINAL_IMAGE = plt.imread(\"UU_LOGO.png\")\n",
    "# Convert to one color channel (using only the red channel), with white background\n",
    "IMAGE = ORIGINAL_IMAGE[:, :, 0] * ORIGINAL_IMAGE[:, :, 3] + (1.0 - ORIGINAL_IMAGE[:, :, 3])\n",
    "\n",
    "\n",
    "# Get a \"camera view\" at the position indicated by state\n",
    "# Use reshape=True to format the output as a data point for the neural network\n",
    "def get_window(state: State, reshape=False) -> np.ndarray:\n",
    "    # When indexing the image as an array, switch the coordinates: im[state[1], state[0]]\n",
    "    window = IMAGE[(state[1] - 14):(state[1] + 14), (state[0] - 14):(state[0] + 14)]\n",
    "    if reshape:\n",
    "        return np.reshape(window, (1, 28, 28, 1))\n",
    "    return window\n",
    "\n",
    "\n",
    "# Is the state close enough to the goal to be considered a success?\n",
    "# There is a margin for error, so that the agent can't jump over the goal\n",
    "def is_goal_reached(state: State) -> bool:\n",
    "    return np.amax(np.abs(np.asarray(state) - np.asarray(GOAL))) <= AVG_MOVEMENT_SIZE / 2 + 1\n",
    "\n",
    "\n",
    "# This is a helper function to render a run\n",
    "def updatefig(j, images, imgplot, text_act_plot, text_reward_plot):\n",
    "    # set the data in the axesimage object\n",
    "    img, time_point, from_state, to_state, act, current_reward = images[min(len(images), j)]\n",
    "    imgplot.set_data(img)\n",
    "    text_act_plot.set_text(f\"Time step: {time_point} - Action: {act}\\nState: {from_state} -> {to_state}\")\n",
    "    text_reward_plot.set_text(f\"Current total reward: {current_reward}\")\n",
    "    # return the artists set\n",
    "    return [imgplot, text_act_plot]\n",
    "\n",
    "\n",
    "# This will render a run of a full epoch\n",
    "# The function needs a list of tuples containing an image array, a State, the performed action\n",
    "def render_epoch(animation_data: List[Tuple[np.ndarray, State, Actions]], interval=100, blit=True, **kwargs):\n",
    "    if not len(animation_data):\n",
    "        return f\"No images in the list\"\n",
    "    fig, ax = plt.subplots()\n",
    "    imgplot = ax.imshow(np.zeros_like(animation_data[0][0]))\n",
    "    text_act_plot = ax.set_title(\"\", color=\"red\", fontweight=\"extra bold\", loc=\"left\")\n",
    "    text_reward_plot = ax.text(5, 255, \"\", color=\"red\", fontweight=\"extra bold\")\n",
    "    params = [animation_data, imgplot, text_act_plot, text_reward_plot]\n",
    "    ani = FuncAnimation(fig,\n",
    "                        updatefig,\n",
    "                        fargs=params,\n",
    "                        frames=len(animation_data),\n",
    "                        interval=interval,\n",
    "                        blit=blit,\n",
    "                        **kwargs)\n",
    "    animation = HTML(ani.to_jshtml())\n",
    "    plt.close()\n",
    "    return display.display(animation)\n",
    "\n",
    "# This function can be used to smooth obtained plots\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts) / box_pts\n",
    "    return np.convolve(y, box, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following 2 images show:\n",
    " * The original image, with a red dot marking the goal and a red rectangle marking the area where the center of agent must remain. A movement that would take the agent outside this rectangle, places him at the boundary instead. The blue rectangle represents an augmented area that is not necessary in the assignement but with which you can play.\n",
    " * What the agent sees if s/he is exactly at the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(IMAGE[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "# Plotting uses reversed y-axis now: larger y values are further down\n",
    "goal_container = plt.plot(GOAL[0], GOAL[1], 'rx', markersize=\"7\")\n",
    "legend2 = plt.legend(goal_container, [\"Goal\"], loc=3)\n",
    "\n",
    "plt.plot([Boundary.WEST, Boundary.WEST, Boundary.EAST, Boundary.EAST, Boundary.WEST],\n",
    "         [Boundary.NORTH, Boundary.SOUTH, Boundary.SOUTH, Boundary.NORTH, Boundary.NORTH],\n",
    "         'r-',\n",
    "         label=\"Movevable area\")\n",
    "plt.plot([AugmentedArea.WEST, AugmentedArea.WEST, AugmentedArea.EAST, AugmentedArea.EAST, AugmentedArea.WEST],\n",
    "         [AugmentedArea.NORTH, AugmentedArea.SOUTH, AugmentedArea.SOUTH, AugmentedArea.NORTH, AugmentedArea.NORTH],\n",
    "         'b-',\n",
    "         label=\"Viewable area\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().add_artist(legend2)\n",
    "plt.show()\n",
    "\n",
    "# window around goal\n",
    "img_container = plt.imshow(get_window(GOAL),\n",
    "                           cmap='gray',\n",
    "                           vmin=0,\n",
    "                           vmax=1.0,\n",
    "                           extent=(GOAL[0] - ACCEPTABLE_DISTANCE_TO_GOAL, GOAL[0] + ACCEPTABLE_DISTANCE_TO_GOAL,\n",
    "                                   GOAL[1] + ACCEPTABLE_DISTANCE_TO_GOAL, GOAL[1] - ACCEPTABLE_DISTANCE_TO_GOAL))\n",
    "plt.plot(GOAL[0], GOAL[1], 'ro', linewidth=1)\n",
    "plt.title(\"Acceptance area around goal\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class provides the functionality of tile encoding. The implementation can be used to define multiple tilings. The default code uses three tilings. You can play with different number of tilings, but please deliver the results with only one setting with multiple tilings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_tilings():\n",
    "    def __init__(self, number_of_grids = 1, offsets = np.array([[0.0, 0.0]])) -> None:       \n",
    "        # Low value for each dimension, for each grid / tile\n",
    "        lows = np.array([[Boundary.NORTH, Boundary.WEST]]*number_of_grids)\n",
    "        # High value for each dimension, for each grid / tile\n",
    "        highs = np.array([[Boundary.SOUTH, Boundary.EAST]]*number_of_grids)\n",
    "        # Number of discrete bin for each each dimension, for each grid / tile\n",
    "        bins = np.array([[9, 9]]*number_of_grids)\n",
    "        # The offset is used to setup the overlap of grids\n",
    "        # offsets = np.array([[0.0, 0.0]]) is one grid starting from lows to highs \n",
    "        # offsets = np.array([[20.0, 20.0]]) is one grid starting from lows+[20.0, 20.0] to highs+[20.0, 20.0]\n",
    "\n",
    "        self.grids = []\n",
    "\n",
    "        for l, h, b, o in zip(lows, highs, bins, offsets):\n",
    "            grid = {}\n",
    "            grid['size']  = b\n",
    "            grid['low'] = l\n",
    "            grid['offset'] = o\n",
    "            grid['points'] = []\n",
    "            grid['step'] = []\n",
    "            for dim in range(len(b)):\n",
    "                points, step = np.linspace(l[dim], h[dim], b[dim]+1, endpoint=False, retstep=True)\n",
    "                points += o[dim]\n",
    "                grid['points'].append(points)\n",
    "                grid['step'].append(step)\n",
    "\n",
    "            grid['step'] = np.array(grid['step'])\n",
    "            grid['weights'] = np.zeros(grid['size'])\n",
    "            self.grids.append(grid)\n",
    "\n",
    "    # Get the sum of the weights for given continuous coordinates\n",
    "    def get_weight(self, sample):\n",
    "        encoded_sample = self.tile_encode(sample)\n",
    "        w = 0.0\n",
    "        for grid, (x,y) in zip(self.grids, encoded_sample):\n",
    "            w += grid['weights'][x,y]\n",
    "        return w\n",
    "\n",
    "    # Set the weights for given continuous coordinates\n",
    "    def set_weight(self, sample, target):\n",
    "        encoded_sample = self.tile_encode(sample)\n",
    "        for grid, (x,y) in zip(self.grids, encoded_sample):\n",
    "            grid['weights'][x,y] = target/len(self.grids)\n",
    "\n",
    "    # Return the discrete coordinates from continuous coordinates for all grids\n",
    "    def tile_encode(self, sample):\n",
    "        encoded_sample = []\n",
    "        for grid in self.grids:\n",
    "            encoded_sample.append(self.discretize(sample, grid))\n",
    "        return encoded_sample    \n",
    "    \n",
    "    # Return the discrete coordinates from continuous coordinates for a given grid\n",
    "    def discretize(self, sample, grid):\n",
    "        sample = np.array(sample) - (grid['low'] + grid['offset'])\n",
    "        sample = np.maximum(sample, np.array([0]*len(grid['size'])))\n",
    "        index = sample // grid['step']\n",
    "        index = np.minimum(index, grid['size']-1)\n",
    "        \n",
    "        return list(index.astype(int))\n",
    "\n",
    "    # Plot the different grids\n",
    "    def visualize_tilings(self):\n",
    "        \"\"\"Plot each tiling as a grid.\"\"\"\n",
    "        prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "        colors = prop_cycle.by_key()['color']\n",
    "        linestyles = ['-', '--', ':']\n",
    "        legend_lines = []\n",
    "        \n",
    "        for i, grid in enumerate(self.grids):\n",
    "            for x in grid['points'][0]:\n",
    "                l = plt.axvline(x=x, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)], label=i)\n",
    "            for y in grid['points'][1]:\n",
    "                l = plt.axhline(y=y, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)])\n",
    "            legend_lines.append(l)\n",
    "        plt.legend(legend_lines, [\"Tiling #{}\".format(t) for t in range(len(legend_lines))], facecolor='white', framealpha=0.9)\n",
    "        plt.title(\"Tilings\")\n",
    "\n",
    "\n",
    "# default setting for 3 tilings:\n",
    "offsets = np.array([[0.0, 0.0], [20.0, 20.0], [-20.0, 15.0]])\n",
    "tilings = grid_tilings(3, offsets)\n",
    "\n",
    "#TEST more tilings\n",
    "# offsets = np.array([ [0.0, 0.0], [20.0, 20.0], [-20.0, 15.0],[10.0, -10.0], [-10.0, -20.0],\n",
    "#    [15.0,  10.0], [-15.0, -10.0], [25.0, -25.0], [-25.0, 25.0], [5.0,   5.0] ])\n",
    "#tilings = grid_tilings(number_of_grids=10, offsets=offsets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example for one tiling\n",
    "# tilings = grid_tilings()\n",
    "\n",
    "plt.imshow(IMAGE[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "tilings.visualize_tilings()\n",
    "\n",
    "# Test with some sample values\n",
    "samples = np.random.rand(5, 2) * 264\n",
    "print(\"\\nSamples:\", samples, sep=\"\\n\")\n",
    "encoded_samples = [tilings.tile_encode(sample) for sample in samples]\n",
    "print(\"\\nIndexes of samples:\", *[s for s in encoded_samples], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following functions complete the definition of the environment. The agent's movements always go in the intended direction, but the distance travelled has a small random component. Besides by reaching the goal, the episode also terminates after TIME_LIMIT (200) steps; at that point, the agent gets a negative reward TIMEOUT_REWARD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gymnasium.Env):\n",
    "    metadata = {'render.modes': ['human', 'rgba_array']}\n",
    "    bx = np.array([AugmentedArea.WEST, AugmentedArea.WEST, AugmentedArea.EAST, AugmentedArea.EAST, AugmentedArea.WEST])\n",
    "    by = np.array([AugmentedArea.NORTH, AugmentedArea.SOUTH, AugmentedArea.SOUTH, AugmentedArea.NORTH, AugmentedArea.NORTH])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_actions = Actions\n",
    "        self.action_space = spaces.Discrete(len(self.num_actions))\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        self.display = None\n",
    "        self.img, self.img_container = Environment._init_visual_area(IMAGE)\n",
    "        self.time = 0\n",
    "\n",
    "    def seed(self, seed=None) -> int:\n",
    "        np.random.seed(seed)\n",
    "        return seed\n",
    "\n",
    "    def step(self, action: Actions):\n",
    "        assert self.action_space.contains(action)\n",
    "        (x, y), was_invalid = self._validate_state(self._move(self.state, action))\n",
    "\n",
    "        self.state = (x, y)\n",
    "        reward = MOVE_REWARD if not was_invalid else INVALID_MOVE_REWARD\n",
    "        reward = TIMEOUT_REWARD if self.time >= TIME_LIMIT else reward\n",
    "        reward = self.time * reward\n",
    "        done = is_goal_reached(self.state) or self.time >= TIME_LIMIT\n",
    "        self.time += 1\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self, state: State = None) -> State:\n",
    "        self.state = self.starting_state() if not state else state\n",
    "        self.time = 0\n",
    "        return self.state\n",
    "\n",
    "    # returns the current environment situation\n",
    "    def render(self, mode='rgba_array'):\n",
    "        curr_img = np.array(self.img_container.get_array())\n",
    "        x, y = self.state\n",
    "        scaler = 4\n",
    "        w, e, n, s = x - scaler, x + scaler, y - scaler, y + scaler\n",
    "        curr_img[n:s, w:e, 0] = 255\n",
    "        curr_img[n:s, w:e, 1] = 0\n",
    "        curr_img[n:s, w:e, 2] = 255\n",
    "        curr_img[n:s, w:e, 3] = 255\n",
    "        cropped_img = curr_img  # Just for debugging purposes\n",
    "        if mode == 'rgba_array':\n",
    "            plt.close()\n",
    "            return cropped_img  # return RGB frame suitable for video\n",
    "        elif mode == 'human':\n",
    "            container = plt.imshow(curr_img)\n",
    "            ax = container.axes\n",
    "            ax.set_xlim(Boundary.WEST, Boundary.EAST, auto=None)\n",
    "            ax.set_ylim(Boundary.SOUTH, Boundary.NORTH, auto=None)\n",
    "            return container\n",
    "        else:\n",
    "            raise Exception(f\"Please specify either 'rgba_array' or 'human' as mode parameter!\")\n",
    "\n",
    "    # Return a randomly chosen non-terminal state as starting state\n",
    "    def starting_state(self) -> State:\n",
    "        while True:\n",
    "            state = (\n",
    "                np.random.randint(Boundary.WEST, Boundary.EAST + 1),\n",
    "                np.random.randint(Boundary.NORTH, Boundary.SOUTH + 1),\n",
    "            )\n",
    "            if not is_goal_reached(state):\n",
    "                return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _move(state: State, action: Actions) -> State:\n",
    "        x, y = state\n",
    "        if action == Actions.NORTH:\n",
    "            y -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.SOUTH:\n",
    "            y += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.WEST:\n",
    "            x -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.EAST:\n",
    "            x += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_state(state: State) -> Tuple[State, bool]:\n",
    "        x, y = state\n",
    "        is_invalid = False\n",
    "        if y < Boundary.NORTH:\n",
    "            is_invalid = True\n",
    "            y = int(Boundary.NORTH)\n",
    "        if y > Boundary.SOUTH:\n",
    "            is_invalid = True\n",
    "            y = int(Boundary.SOUTH)\n",
    "        if x < Boundary.WEST:\n",
    "            is_invalid = True\n",
    "            x = int(Boundary.WEST)\n",
    "        if x > Boundary.EAST:\n",
    "            is_invalid = True\n",
    "            x = int(Boundary.EAST)\n",
    "        return (x, y), is_invalid\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_visual_area(img) -> np.ndarray:\n",
    "        x, y = img.shape\n",
    "        my_dpi = 80\n",
    "        fig = Figure(figsize=(y / my_dpi, x / my_dpi), dpi=my_dpi)\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        ax.plot(GOAL[0], GOAL[1], 'ro', linewidth=5)\n",
    "        ax.plot(Environment.bx, Environment.by, 'b-')\n",
    "        img_container = ax.imshow(img[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "        fig.tight_layout()\n",
    "        canvas.draw()  # draw the canvas, cache the renderer\n",
    "        s, (width, height) = canvas.print_to_buffer()\n",
    "        image = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n",
    "        img_container.set_data(image)\n",
    "        plt.close()\n",
    "        return image, img_container\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement your agent\n",
    "\n",
    "Next comes your part. The following class is responsible for the agent's behavior. The select_action function should implement the epsilon-greedy policy, and return an action chosen according to that policy. **Please fill in the missing codes in select_action function (1.5 points).** \n",
    "\n",
    " Remark: This is an abstract class.\n",
    " Hence, its sole purpose is creating subclasses from it, which is also the reason it cannot be instantiated.\n",
    " The following subsequent subclasses will provide a specific implementation for the methods that are missing here.\n",
    " Therefore, you can ignore the functions that are not implemented. This is just a common way to make sure that all subclasses behave similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        # set up the value of epsilon\n",
    "        self.alpha = alpha  # learning rate or step size\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.hist = []\n",
    "\n",
    "    # Choose action at state based on epsilon-greedy policy and valueFunction\n",
    "    def select_action(self, state: State, use_greedy_strategy: bool = True) -> int:\n",
    "\n",
    "        # TO BE FILLED (1.5 points)\n",
    "        # get the values \n",
    "        estimated_action_values = self.values(state)\n",
    "\n",
    "        if use_greedy_strategy:\n",
    "            action = np.argmax(estimated_action_values) # always explore\n",
    "        else:\n",
    "            if (np.random.rand() < self.epsilon):\n",
    "                action = np.random.randint(len(estimated_action_values)) # explore\n",
    "            else:\n",
    "                action = np.argmax(estimated_action_values) # exploit\n",
    "\n",
    "                  \n",
    "        self.hist.append(estimated_action_values)\n",
    "        return action\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    @abstractmethod\n",
    "    def value(self, state: State, action: Actions) -> float:\n",
    "        pass\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    @abstractmethod\n",
    "    def values(self, state: State) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    # Set value for given state and action\n",
    "    @abstractmethod\n",
    "    def set_value(self, state: State, action: Actions):\n",
    "        pass\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy\n",
    "    @abstractmethod\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False) -> int:\n",
    "        return None\n",
    "\n",
    "    # Return estimated state value, based on the estimated action values\n",
    "    def state_value(self, state):\n",
    "        return np.max(self.values(state))\n",
    "\n",
    "    # Plot the state value estimates. Use a larger stride for lower resolution.\n",
    "    def plot_state_values(self, stride=1):\n",
    "        self.v = np.zeros(\n",
    "            ((Boundary.SOUTH - Boundary.NORTH + stride) // stride, (Boundary.EAST - Boundary.WEST + stride) // stride))\n",
    "        for j, x in enumerate(range(Boundary.WEST, Boundary.EAST + 1, stride)):\n",
    "            for i, y in enumerate(range(Boundary.NORTH, Boundary.SOUTH + 1, stride)):\n",
    "                self.v[i, j] = self.state_value((x, y))\n",
    "\n",
    "        plt.imshow(self.v)\n",
    "        plt.colorbar()\n",
    "        return plt.show()\n",
    "\n",
    "    def plot_q_values(self, skip=1):\n",
    "        return pd.DataFrame(self.hist[::skip]).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next classes are two agents using either episodic semi-gradient Q-learning and episodic semi-gradient SARSA algorithm to estimate the value function. Both agents use the same linear function approximation method with tile coding. **Implement the `learn` function according to the update rule for the respective algorithm (1 point for each)**. \n",
    " \n",
    " REMARK: Both agents use the same tile coding. This method helps splitting the state-space into discrete chunks. Each chunk is associated with one weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles the function approximation, with several methods to query and update it. \n",
    "# A linear approximation function is used, making the computation much faster.\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "        # Use a tile coding\n",
    "        self.tilings = [grid_tilings(number_of_grids=3) for a in Actions]\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "\n",
    "    # Set value for given state and action\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        self.tilings[action].set_weight(state, value)\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy: for Qlearning, the agent does not need to return the next selected action\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False):\n",
    "        # TO BE FILLED (1 point)\n",
    "        # get the current estimated action value\n",
    "        estimated_action_value = self.value(state, action)\n",
    "        \n",
    "        if done: # set target excluding next state's value\n",
    "            target = reward \n",
    "        else: # compute target by including the next state's value\n",
    "            target = reward + self.gamma * np.max(self.values(next_state))\n",
    "        \n",
    "        # compute TD error\n",
    "        temporal_difference = target - estimated_action_value\n",
    "        \n",
    "        # update action value\n",
    "        new_value = estimated_action_value + self.alpha * temporal_difference\n",
    "        self.set_value(state, action, new_value)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "\n",
    "        # Use a tile coding\n",
    "        self.tilings = [grid_tilings(number_of_grids=3) for a in Actions]\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "\n",
    "    # Set value for given state and action\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        self.tilings[action].set_weight(state, value)\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy: for SARSA, the agent needs to return the next selected action\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False):\n",
    "        # TO BE FILLED (1 point)\n",
    "        # get the current estimated action value\n",
    "        estimated_action_value = self.value(state, action)\n",
    "\n",
    "        if done:\n",
    "            # terminal state\n",
    "            next_action = None\n",
    "            target = reward\n",
    "        else:\n",
    "            # Select next action (on-policy)\n",
    "            next_action = self.select_action(next_state, use_greedy_strategy=False)\n",
    "            target = reward + self.gamma * self.value(next_state, next_action)\n",
    "        \n",
    "        # compute TD error\n",
    "        temporal_difference = target - estimated_action_value\n",
    "\n",
    "        # update action value\n",
    "        new_value = estimated_action_value + self.alpha * temporal_difference\n",
    "        self.set_value(state, action, new_value)\n",
    "\n",
    "        return next_action\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following function handles the interaction between agent and environment for a single episode. By passing the same value_function object in multiple calls to this function, the agent can learn from a sequence of such interactions.\n",
    "\n",
    " **Please fill in the missing parts (1 point).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: Environment in which the agent is supposed to run \n",
    "# agent: agent to learn\n",
    "# initital state: Starting state for the environment\n",
    "# is_learning: should the value function be updated during the simulation?\n",
    "# is_rendering: should the run be recorded? (Takes some time to compute)\n",
    "\n",
    "\n",
    "def run_episode(env: Environment,\n",
    "                agent: Agent,\n",
    "                initial_state: State,\n",
    "                is_learning: bool = True,\n",
    "                is_rendering: bool = False) -> Tuple[State, float]:\n",
    "    # Initialize reward for episode\n",
    "    total_reward = 0.0\n",
    "    # Initialize the policy (if is_greedy=True then the agent follows its optimal policy, otherwise it will randomly select an action)\n",
    "    is_greedy = not is_learning\n",
    "    # Get initial action\n",
    "    current_state = initial_state\n",
    "    current_action = agent.select_action(initial_state, use_greedy_strategy=is_greedy)\n",
    "\n",
    "    # Track the rendering\n",
    "    animation_data = []\n",
    "    animation_data.append((env.render(), env.time, None, current_state, None, 0))\n",
    "    # Initialize variables\n",
    "    next_state = None\n",
    "    \n",
    "    \n",
    "    done = False\n",
    "    next_action = None\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(current_action)\n",
    "\n",
    "\n",
    "        total_reward += reward\n",
    "        if is_rendering:\n",
    "            curr_img = env.render()\n",
    "            animation_data.append((curr_img, env.time, current_state, next_state, current_action, total_reward))\n",
    "        \n",
    "        # Execute the learning and update the state and action\n",
    "        # TO BE FILLED (1 point)\n",
    "        \n",
    "        if is_learning:\n",
    "            next_action = agent.learn(current_state, current_action, next_state, reward, done)\n",
    "        \n",
    "        # if agent is instance of QAgent or learning is off, then select the next action\n",
    "        if not is_learning or next_action is None:\n",
    "            next_action = agent.select_action(next_state, use_greedy_strategy=is_greedy)\n",
    "        \n",
    "        # update state and action\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "            \n",
    "    agent.last_action = None\n",
    "    return current_state, total_reward, animation_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To help understand your agent, you can render the agent's performance by setting render to True and running the `run_epoch` function. \n",
    " \n",
    " There are some helper functions. They might help you implement the agent correctly. \n",
    " * `agent.plot_state_values` shows you how the agent values different states\n",
    " * `agent.plot_q_values` shows the q_values that the agent had over the course of his life time. (That could be a lot. There's a skip parameter to reduce the amount of data points)\n",
    "\n",
    " REMARK: Keep in mind, the following is just one example run. Don't expect the model to be fully trained after just one episode. The training part follows in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "epsilon = 0.0\n",
    "gamma = .7\n",
    "alpha = 1e-2\n",
    "agent = QAgent(alpha, gamma, epsilon)\n",
    "start_state = env.reset()\n",
    "end_state, total_reward, animation_data = run_episode(env, agent, start_state, is_learning=True, is_rendering=True)\n",
    "agent.plot_state_values()\n",
    "agent.plot_q_values(skip=1)\n",
    "render_epoch(animation_data, interval=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the simulation, play with parameters and analyse results\n",
    "\n",
    " Now it's time to train both algorithms/agents on the environment. \n",
    " \n",
    " In the simulations, please plot the measure of each algorithm's performance as a function of the episode (i.e. the sum of all immediate rewards received in each episode). You shall play with a few combinations of two parameters discounted factor $\\gamma$ and stepsize $\\alpha$ (at least two variables for each parameter). During the experiments, keep $\\epsilon$ fixed at $0.01$. A reasonable starting point for $\\alpha$ is 1e-2. \n",
    " \n",
    " REMARK: You can save the parameters and update-to-date Q values of each agent, so that you can still test their performance later. (You can achieve this by keeping the object.)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Please submit your code as well as the plots presenting compariable performance of the different combinations of parameters for every algorithms (2 points)**. \n",
    "\n",
    "REMARK: For a decent comparison all agents should be plotted on the same axis bounds. Also, the plots may be hard to interpret because of the scales. Feel free to do your own smoothing or use the `smooth` function provided in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare parameter grid for tested param combinations\n",
    "param_grid = {\n",
    "    \"num_episodes\" : [500],\n",
    "    \"epsilon\": [1e-2],\n",
    "    \"discounted_factor\": [0.5, 0.7, 0.9], # relative -> moderate -> strong focus on future rewards\n",
    "    \"stepsize\": [1e-2, 5e-2, 0.1], # relative -> moderate -> aggressive learning,\n",
    "    \"is_learning\": [True, False],\n",
    "    \"agent\": [SARSAAgent, QAgent]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(num_episodes: int, agent: Agent, is_learning: bool) -> List[float]:\n",
    "    \"\"\"Simulate multiple episodes of the interaction between an agent and the environment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_episodes: int\n",
    "        The number of episodes the simulation should run for\n",
    "    agent: Agent\n",
    "        The agent which will be used to interact with the environment\n",
    "    is_learning: bool\n",
    "        Whether the agent learns during the run of episodes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        List with total rewards for each episode from the simulation\n",
    "    \"\"\"\n",
    "    env = Environment()\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        start_state = env.reset()\n",
    "        _, total_reward, _ = run_episode(env=env, agent=agent, initial_state=start_state,\n",
    "                                                              is_learning=is_learning, is_rendering=False)\n",
    "        rewards.append(total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve readability during indexing\n",
    "\n",
    "class ResultsKey(NamedTuple):\n",
    "    num_episodes: int\n",
    "    epsilon: float\n",
    "    discounted_factor: float\n",
    "    stepsize: float\n",
    "    is_learning: bool\n",
    "    agent: Agent\n",
    "\n",
    "Result = Dict[ResultsKey, List[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations(param_grid: Dict[str, List[Union[float, int, type]]]) -> Result:\n",
    "    \"\"\"Runs simulations for all combinations within the provided parameter grid\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    param_grid: Dict[str, List[Union[float, int, type]]]\n",
    "        Dictionary with lists of values for the parameters: `agent` (type), \n",
    "        `num_episodes`, `epsilon`, `discounted_factor`, `stepsize`\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Result\n",
    "        A dictionary with the parameters for each simulation as keys and the rewards for each simulation as values\n",
    "\n",
    "    \"\"\"\n",
    "    # Store rewards for each simulation\n",
    "    results: Result = {}\n",
    "\n",
    "    # create all possible param combinations\n",
    "    param_combis = [\n",
    "        dict(zip(param_grid.keys(), param_combi))\n",
    "        for param_combi in product(*param_grid.values())\n",
    "    ]\n",
    "\n",
    "    for params in param_combis:\n",
    "        if isinstance(params[\"agent\"], tuple): # Ugly fix for fixed agent classes\n",
    "            agent = params[\"agent\"][1]()\n",
    "        else:\n",
    "            agent = params[\"agent\"]( # init agent\n",
    "                alpha=params[\"stepsize\"],\n",
    "                gamma=params[\"discounted_factor\"],\n",
    "                epsilon=params[\"epsilon\"]\n",
    "            )\n",
    "\n",
    "        rewards = simulate(num_episodes=params[\"num_episodes\"], agent=agent, is_learning=params[\"is_learning\"])\n",
    "        # Store the initiated agent for later retrieving info\n",
    "        params[\"agent\"] = agent\n",
    "        results[ResultsKey(**params)] = rewards # Store rewards for each simulation\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_simulations(param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smooth(data: List[float], alpha=0.1) -> List[float]:\n",
    "    \"\"\"\n",
    "    Exponential smoothing for a given list of data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: List[float]\n",
    "        the data which is to be smoothed\n",
    "    alpha: float, optional\n",
    "        smoothing factor (0 < alpha <= 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        Smoothed data\n",
    "    \"\"\"\n",
    "    smoothed = []\n",
    "    s = data[0]  \n",
    "    for x in data:\n",
    "        s = alpha * x + (1 - alpha) * s\n",
    "        smoothed.append(s)\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation_results(results: Result):\n",
    "    \"\"\"Plots the results of the learning (vs non-learning) QLearning and SARSA agents\"\"\"\n",
    "    # Extract unique combis of discounted_factor and stepsize\n",
    "    unique_discountf_2_steps = set((keys.discounted_factor, keys.stepsize) for keys in results.keys())\n",
    "    \n",
    "    # For each discounted_factor and stepsize combination plot learning algorithms (vs non-learning algorithms)\n",
    "    for discount_factor, step_size in unique_discountf_2_steps:\n",
    "        # Retrieve learning results with discount_factor and step_size\n",
    "        learning_rewards = {key.agent: results[key]\n",
    "                            for key in results\n",
    "                            if key.discounted_factor == discount_factor and\n",
    "                            key.stepsize == step_size and key.is_learning}\n",
    "        # Retrieve non-learning results with discount_factor and step_size\n",
    "        non_learning_rewards = {key.agent: results[key]\n",
    "                                for key in results\n",
    "                                if key.discounted_factor == discount_factor and \n",
    "                                key.stepsize == step_size and not key.is_learning}\n",
    "\n",
    "        \n",
    "        titles = []\n",
    "        data = []\n",
    "        if bool(learning_rewards):\n",
    "            titles += [\"Learning Algorithms\"]\n",
    "            data += [learning_rewards]\n",
    "        if bool(non_learning_rewards):\n",
    "            titles += [\"Non-Learning Algorithms\"]\n",
    "            data += [non_learning_rewards]\n",
    "\n",
    "        # Determine whether there is a need for 2 plots for each axes\n",
    "        ncols = len(titles)\n",
    "        figsize = (14,6) if len(titles) > 1 else (7,6)\n",
    "\n",
    "        # Setup figure\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=ncols, figsize=figsize)\n",
    "        axs = [axs] if ncols == 1 else axs\n",
    "        \n",
    "        colours = [(0.0, 0.0, 0.9), (0.9,0.3,0.3)] # blue, red respectively\n",
    "\n",
    "        # Plot the learning algorithms on the left and non-learning algorithms on the right\n",
    "        for i, (rewards, title) in enumerate(zip(data, titles)):\n",
    "            ax = axs[i]\n",
    "\n",
    "            # Plot the rewards for each of the agents\n",
    "            for j, (agent, reward) in enumerate(rewards.items()):\n",
    "                rewards_smoothed = exponential_smooth(data=reward)\n",
    "\n",
    "                if title == \"Learning Algorithms\":\n",
    "                    ax.plot(reward, alpha=0.3, label=f\"{type(agent).__name__}\", color=colours[j])\n",
    "                ax.plot(rewards_smoothed, label=f\"{type(agent).__name__} (smoothed)\", color=colours[j])\n",
    "\n",
    "            # Style plot\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(\"Episodes\")\n",
    "            ax.set_ylabel(\"Sum of rewards during episode\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "        plt.suptitle(f\"Performance of Q-Learning and SARSA with\\n($\\\\gamma={discount_factor}$, $\\\\alpha={step_size}$)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_simulation_results(results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your plots here. (In case your code takes too long to run.) \n",
    "\n",
    "<blockquote>\n",
    "<p><b>PLOTS OF LEARNING ALGORITHMS</b></p>\n",
    "<p>From left to right the plots of the algorithms can be read as having a relative, moderate, and strong focus on future rewards. From top to bottom, the plots can be read as the algorithms used a relative &rightarrow; moderate &rightarrow; aggressive learning approach,</p>\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 10px;\">\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "<p>DISCOUNTED FACTOR (&gamma;) = 0.5</p>\n",
    "<p>Relative focus on future awards</p>\n",
    "<img src=\"img/performance-learning-discounted-factor-5e-1-stepsize-1e-2.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "<img src=\"img/performance-learning-discounted-factor-5e-1-stepsize-5e-2.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "<img src=\"img/performance-learning-discounted-factor-5e-1-stepsize-1e-1.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "<p>DISCOUNTED FACTOR (&gamma;) = 0.7</p>\n",
    "<p>Moderate focus on future awards</p>\n",
    "<img src=\"img/performance-learning-discounted-factor-7e-1-stepsize-1e-2.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "<img src=\"img/performance-learning-discounted-factor-7e-1-stepsize-5e-2.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "<img src=\"img/performance-learning-discounted-factor-7e-1-stepsize-1e-1.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "<p>DISCOUNTED FACTOR (&gamma;) = 0.9</p>\n",
    "<p>Strong focus on future awards</p>\n",
    "<img src=\"img/performance-learning-discounted-factor-9e-1-stepsize-1e-2.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "<img src=\"img/performance-learning-discounted-factor-9e-1-stepsize-5e-2.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "<img src=\"img/performance-learning-discounted-factor-9e-1-stepsize-1e-1.png\" style=\"width: 100%; margin-bottom: 10px;\">\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Please interpret the results/plots (2 points)**. Explain how two algorithms differ in the learning procedure from another and what the results say about the parameters alpha (step-size) and gamma (decay-rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INTERPRETATION OF PLOTS**\n",
    "> 1. _**Difference in learning procedure: Q-Learning Agent vs. SARSA Agent**_  \n",
    "> The Q-Learning Agent deploys the Q-Learning algorithm, which is an off-policy algorithm, while the SARSA agent, uses the SARSA algorithm, which is an on-policy algorithm. Both Q-Learning and SARSA use the $\\varepsilon$-greedy policy to choose their next action (unless they act fully greedy).   \n",
    "> \n",
    ">       The difference between the algorithms lies in their way of updating the q-value:  \n",
    "> * The Q-Learning algorithm always exploits the greedy action for the next state regardless of the action actually taken during exploration. This makes Q-Learning very much focused on long-term rewards by learning the values of optimal actions but leads to less robustness in environments with high variability.\n",
    "> * The SARSA algorithm updates the q-value using the action taken during exploration. This makes the SARSA more stable in dynamic environments because it learns from exploratory actions.  \n",
    "> \n",
    ">   This is reflected by the plots which show that the Q-Learning Agent generally converges faster during the early episodes by always using the greedy action for updates compared to the SARSA agent which is focused on learning from the exploratory actions and thereby exhibiting more stable learning curves. \n",
    ">\n",
    "> 2. _**Interpretation of parameters alpha and gamma from plots**_  \n",
    "> By comparing the plots in a grid-like way (discount rate horizontally and step size vertically), the effects of the discount factor and the stepsize become evident:  \n",
    "> * Impact of the discount factor:\n",
    ">   * When comparing the discount rates, the increasing urge to look-ahead for future rewards helps the agents in more quickly reaching stable convergence. \n",
    ">   * It also shows that there is not a whole lot of increase in the performance of the agents between using a discount factor of 0.7 and 0.9. \n",
    "> * Impact of the step size:\n",
    ">   * Comparing the agents by their learning approach, it shows that the agents more quickly converge by learning faster when the step size is increased. \n",
    ">   * However, increasing the stepsize also leads to overshooting during episodes with large variability which shows by the great fluctuations in the plot. Whereas the QAgent thrives by the strong learning urge, the SARSA agent which works on the principle of stability is more benefitted by a moderate learning urge shown by the slightly more graceful curve in the plots.  \n",
    "> \n",
    ">   The optimal parameters for which both the agents fastest reach convergence during episodes is when they use a strong urge to look ahead for future rewards ($\\gamma=0.9$) and for the SARSA agent a moderate learning approach of 0.05, and an agressive learning approach ($\\alpha=0.1$) for the Q-Learning Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Let us think one step further by looking at the policies you have learned (1.5 points).** \n",
    "\n",
    "Please compare the performance of your learned optimal policies using another simulation. In this simulation, you shall have three agents (Qlearning_Agent, SARSA_agent and Random_agent), where all three agents always use their initial policy to behave and find the goal in the given map (make sure the agent does NOT learn anymore!). The initial policies of Qlearning_Agent and SARSA_agent are the optimal policies learned from the above experiments (based on the final estimated Q-values using Q-learning and SARSA algorithm respectively). The inital policy of Random_Agent will select 4 actions randomly. \n",
    "\n",
    "Describe the performance of three agents by running each agent on the task and discuss the results (1.5 points). You can use the render function provided above (and other helper functions) observe the different behaviors of three agents. You could also use an appropriate plot to show the different performances. *There is no need to submit your codes for this question.*\n",
    "\n",
    "Some questions you shall think about and answer (in case there are no differences or no special things, just indicate what you observed):\n",
    "- How does the learned Q-learning policy and SARSA policy perform differently? Does it show the difference between on-policy and off-policy methods?\n",
    "- What kind of strategy did the two RL agents learn similarly? How do they differ from the random policy?\n",
    "- For the RL-learned policies, do you still observe something that does not perform so well? Can you think about possible ways to improve/solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve optimally performing agents\n",
    "q_agent: QAgent = None\n",
    "sarsa_agent: SARSAAgent = None\n",
    "\n",
    "for key in results:\n",
    "    if key.discounted_factor == 0.9 and key.is_learning:\n",
    "        if key.stepsize == 0.05 and isinstance(key.agent, SARSAAgent):\n",
    "            sarsa_agent = key.agent\n",
    "        elif key.stepsize == 0.1 and isinstance(key.agent, QAgent):\n",
    "            q_agent = key.agent\n",
    "    if q_agent and sarsa_agent:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract q-values\n",
    "q_vals_q_agent = q_agent.tilings\n",
    "q_vals_sarsa_agent = sarsa_agent.tilings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent that has been pre-trained and doesn't learn anymore\n",
    "class FixedAgent(Agent):\n",
    "    def __init__(self, alpha, gamma, epsilon, tilings:List[grid_tilings]):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "        self.tilings = tilings\n",
    "    \n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]  \n",
    "    \n",
    "    def set_value(self, state: State, action: Actions, value: float) -> None:\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state: State, use_greedy_strategy = True) -> Actions:\n",
    "        return super().select_action(state, use_greedy_strategy=True)\n",
    "    \n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done = False) -> None:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent that always chooses its actions randomly\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "        self.tilings = [grid_tilings(number_of_grids=3) for a in Actions]\n",
    "\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "    \n",
    "    def set_value(self, state: State, action: Actions, value: float) -> None:\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state: State, use_greedy_strategy = True) -> Actions:\n",
    "        # estimated action values for the current state\n",
    "        estimated_action_vals = self.values(state) # Has to happen to determine valid actions\n",
    "        action_ix_2_qval = list(enumerate(estimated_action_vals)) \n",
    "        return action_ix_2_qval[np.random.choice(len(action_ix_2_qval))][0]\n",
    "    \n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done = False):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_fixed_agents = {\n",
    "    \"num_episodes\" : [500],\n",
    "    \"epsilon\": [1e-2],\n",
    "    \"discounted_factor\": [0.9],\n",
    "    \"stepsize\": [0.1], # :TODO also include 0.05?\n",
    "    \"is_learning\": [False],\n",
    "    \"agent\": [\n",
    "        (FixedAgent, lambda: FixedAgent(tilings=q_vals_sarsa_agent, alpha=0.1, gamma=0.9, epsilon=0.01)),\n",
    "        (FixedAgent, lambda: FixedAgent(tilings=q_vals_q_agent, alpha=0.1, gamma=0.9, epsilon=0.01)),\n",
    "        RandomAgent\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_agents_results = run_simulations(param_grid=param_grid_fixed_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustment on the plot_simulation_results function to plot the 3 agents in a clear way\n",
    "def plot_comparison_fixed_agents(results: Result):\n",
    "    \"\"\"Plot the performance of a trained SARSAAgent, QAgent, and RandomAgent\"\"\"\n",
    "\n",
    "    labels = [\"Trained SARSA Agent\", \"Trained Q-Learning Agent\", \"Random Agent\"]\n",
    "    colours = [(0.0, 0.0, 0.9), (0.9, 0.3, 0.3), (0.0, 0.9, 0.3)]\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "    for ix, (label, colour) in enumerate(zip(labels, colours)):\n",
    "        \n",
    "        rewards = [results[key]\n",
    "            for i, key in enumerate(results) \n",
    "            if i == ix\n",
    "        ][0]\n",
    "        rewards_smoothed = exponential_smooth(data=rewards)\n",
    "        ax.plot(rewards_smoothed, label=f\"{label} (smoothed)\", color=colour)\n",
    "        \n",
    "    ax.set_title(\"Performance of Trained QAgent and SARSAAgent\\nvs. \\nRandom Agent\")\n",
    "    ax.set_xlabel(\"Episodes\")\n",
    "    ax.set_ylabel(\"Sum of Rewards during Episode\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison_fixed_agents(fixed_agents_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers here.\n",
    "\n",
    "> **_Performance Comparison_**  \n",
    "> ![](img/fixed-qagent-vs-fixed-sarsaagent-vs-randomagent.png)  \n",
    "> As the trained Agents had already mostly converged during the training, it is to be expected that they perform near optimal during the episodes. This is reflected by the plot showing almost perfect horizontal lines where the sum of rewards constantly approaches 0. In contrast, the random agent, which doesn't learn and always randomly chooses actions shows large variations within the sum of rewards between episodes and fails to converge.  \n",
    ">   \n",
    "> While the random agent randomly explores, it still gathers valuable information on state-action pairs and rewards. If a non-learning agent were to use the q-values from a random agent which does update its q-values, it could possibly perform perform well by using a greedy policy. However, for this to work effectively, the random agent must have explored enough of the environment such that, should the agent be randomly be assigned a starting point, it knows enough of its surroundings to decide an optimal path to the goal. This also means that the q-values will most likely be sub-optimal as there is no way of predicting when the agent has explored its environment enough to have a good grasp of which actions lead to the most rewards due to its random action selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost done! Before handing in, make sure that the code you hand in work, and that all plots are shown. **Submit just one file per team.**\n",
    "\n",
    "Again, make sure **you name this file according to your last names**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
