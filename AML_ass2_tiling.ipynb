{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Advanced Machine Learning - programming assignment 2\n",
    "\n",
    "*Due: Monday December 16th* (**NOON**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in:**\n",
    "* Cem Kaya (9276866)\n",
    "* Gaynora van Dommelen (6717659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further instructions:\n",
    "* Code quality is considered during the assessement. Make sure your code is properly commented.\n",
    "* You can find the required python packages in requirements.txt (Keep in mind, the grader most likely won't install additional packages. Try to stick with the standard library and the packages listed). Also, we recommend to use python 3.10. \n",
    "* Submit your code in Blackboard using one of your accounts; we will put the grade in Blackboard for the other team member as well.\n",
    "* **Make sure to name the submitted file according to your and your collaborators last name.** (submitter_collaborator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Reinforcement learning with function approximation\n",
    "\n",
    "In this assignment, you'll design your own agent to complete an episodic MDP task following the gymnasium (gym) framework. The agent will be looking at a small part of the UU logo, and have to decide which of the four compass directions (i.e. left, right, up, down) to move in. The learning task is to find the goal in the center as soon as possible.\n",
    "\n",
    "The learning objectives of this assignment are:\n",
    "\n",
    "- Implement two versions of the agent using Semi-gradient SARSA and Q-learning algorithms with a linear approximation function,\n",
    "- Demonstrate the difference between on-policy and off-policy RL methods,\n",
    "- Learn to integrate the approximation function with Tabular RL methods,\n",
    "- Play with the parameters discount factor $\\gamma$ and stepsize $\\alpha$ and understand their influence on the learning procedure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Let's start with setting up the enviroment.\n",
    "\n",
    "The following code defines various aspects of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import IntEnum\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib.figure import Figure\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm  # For progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Constants defining the environment\n",
    "GOAL = (140, 120)\n",
    "CENTER = (132, 132)\n",
    "AVG_MOVEMENT_SIZE = 24\n",
    "ACCEPTABLE_DISTANCE_TO_GOAL = (AVG_MOVEMENT_SIZE // 2) + 1\n",
    "RADIUS = 72\n",
    "WINDOW_SIZE = 28\n",
    "TIME_LIMIT = 200\n",
    "TIMEOUT_REWARD = -100.0\n",
    "MOVE_REWARD = -1\n",
    "INVALID_MOVE_REWARD = -5\n",
    "\n",
    "\n",
    "# This is for type inference\n",
    "State = Tuple[int, int]\n",
    "\n",
    "\n",
    "# Action space\n",
    "class Actions(IntEnum):\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "\n",
    "# Boundaries\n",
    "class Boundary(IntEnum):\n",
    "    WEST = CENTER[0] - RADIUS\n",
    "    EAST = CENTER[0] + RADIUS\n",
    "    NORTH = CENTER[1] - RADIUS\n",
    "    SOUTH = CENTER[1] + RADIUS\n",
    "\n",
    "# Augmented boundarie, not used in this assignment\n",
    "class AugmentedArea(IntEnum):\n",
    "    WEST = Boundary.WEST - (WINDOW_SIZE // 2)\n",
    "    EAST = Boundary.EAST + (WINDOW_SIZE // 2)\n",
    "    NORTH = Boundary.NORTH - (WINDOW_SIZE // 2)\n",
    "    SOUTH = Boundary.SOUTH + (WINDOW_SIZE // 2)\n",
    "\n",
    "\n",
    "# Image\n",
    "ORIGINAL_IMAGE = plt.imread(\"UU_LOGO.png\")\n",
    "# Convert to one color channel (using only the red channel), with white background\n",
    "IMAGE = ORIGINAL_IMAGE[:, :, 0] * ORIGINAL_IMAGE[:, :, 3] + (1.0 - ORIGINAL_IMAGE[:, :, 3])\n",
    "\n",
    "\n",
    "# Get a \"camera view\" at the position indicated by state\n",
    "# Use reshape=True to format the output as a data point for the neural network\n",
    "def get_window(state: State, reshape=False) -> np.ndarray:\n",
    "    # When indexing the image as an array, switch the coordinates: im[state[1], state[0]]\n",
    "    window = IMAGE[(state[1] - 14):(state[1] + 14), (state[0] - 14):(state[0] + 14)]\n",
    "    if reshape:\n",
    "        return np.reshape(window, (1, 28, 28, 1))\n",
    "    return window\n",
    "\n",
    "\n",
    "# Is the state close enough to the goal to be considered a success?\n",
    "# There is a margin for error, so that the agent can't jump over the goal\n",
    "def is_goal_reached(state: State) -> bool:\n",
    "    return np.amax(np.abs(np.asarray(state) - np.asarray(GOAL))) <= AVG_MOVEMENT_SIZE / 2 + 1\n",
    "\n",
    "\n",
    "# This is a helper function to render a run\n",
    "def updatefig(j, images, imgplot, text_act_plot, text_reward_plot):\n",
    "    # set the data in the axesimage object\n",
    "    img, time_point, from_state, to_state, act, current_reward = images[min(len(images), j)]\n",
    "    imgplot.set_data(img)\n",
    "    text_act_plot.set_text(f\"Time step: {time_point} - Action: {act}\\nState: {from_state} -> {to_state}\")\n",
    "    text_reward_plot.set_text(f\"Current total reward: {current_reward}\")\n",
    "    # return the artists set\n",
    "    return [imgplot, text_act_plot]\n",
    "\n",
    "\n",
    "# This will render a run of a full epoch\n",
    "# The function needs a list of tuples containing an image array, a State, the performed action\n",
    "def render_epoch(animation_data: List[Tuple[np.ndarray, State, Actions]], interval=100, blit=True, **kwargs):\n",
    "    if not len(animation_data):\n",
    "        return f\"No images in the list\"\n",
    "    fig, ax = plt.subplots()\n",
    "    imgplot = ax.imshow(np.zeros_like(animation_data[0][0]))\n",
    "    text_act_plot = ax.set_title(\"\", color=\"red\", fontweight=\"extra bold\", loc=\"left\")\n",
    "    text_reward_plot = ax.text(5, 255, \"\", color=\"red\", fontweight=\"extra bold\")\n",
    "    params = [animation_data, imgplot, text_act_plot, text_reward_plot]\n",
    "    ani = FuncAnimation(fig,\n",
    "                        updatefig,\n",
    "                        fargs=params,\n",
    "                        frames=len(animation_data),\n",
    "                        interval=interval,\n",
    "                        blit=blit,\n",
    "                        **kwargs)\n",
    "    animation = HTML(ani.to_jshtml())\n",
    "    plt.close()\n",
    "    return display.display(animation)\n",
    "\n",
    "# This function can be used to smooth obtained plots\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts) / box_pts\n",
    "    return np.convolve(y, box, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following 2 images show:\n",
    " * The original image, with a red dot marking the goal and a red rectangle marking the area where the center of agent must remain. A movement that would take the agent outside this rectangle, places him at the boundary instead. The blue rectangle represents an augmented area that is not necessary in the assignement but with which you can play.\n",
    " * What the agent sees if s/he is exactly at the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(IMAGE[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "# Plotting uses reversed y-axis now: larger y values are further down\n",
    "goal_container = plt.plot(GOAL[0], GOAL[1], 'rx', markersize=\"7\")\n",
    "legend2 = plt.legend(goal_container, [\"Goal\"], loc=3)\n",
    "\n",
    "plt.plot([Boundary.WEST, Boundary.WEST, Boundary.EAST, Boundary.EAST, Boundary.WEST],\n",
    "         [Boundary.NORTH, Boundary.SOUTH, Boundary.SOUTH, Boundary.NORTH, Boundary.NORTH],\n",
    "         'r-',\n",
    "         label=\"Movevable area\")\n",
    "plt.plot([AugmentedArea.WEST, AugmentedArea.WEST, AugmentedArea.EAST, AugmentedArea.EAST, AugmentedArea.WEST],\n",
    "         [AugmentedArea.NORTH, AugmentedArea.SOUTH, AugmentedArea.SOUTH, AugmentedArea.NORTH, AugmentedArea.NORTH],\n",
    "         'b-',\n",
    "         label=\"Viewable area\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().add_artist(legend2)\n",
    "plt.show()\n",
    "\n",
    "# window around goal\n",
    "img_container = plt.imshow(get_window(GOAL),\n",
    "                           cmap='gray',\n",
    "                           vmin=0,\n",
    "                           vmax=1.0,\n",
    "                           extent=(GOAL[0] - ACCEPTABLE_DISTANCE_TO_GOAL, GOAL[0] + ACCEPTABLE_DISTANCE_TO_GOAL,\n",
    "                                   GOAL[1] + ACCEPTABLE_DISTANCE_TO_GOAL, GOAL[1] - ACCEPTABLE_DISTANCE_TO_GOAL))\n",
    "plt.plot(GOAL[0], GOAL[1], 'ro', linewidth=1)\n",
    "plt.title(\"Acceptance area around goal\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class provides the functionality of tile encoding. The implementation can be used to define multiple tilings. The default code uses three tilings. You can play with different number of tilings, but please deliver the results with only one setting with multiple tilings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_tilings():\n",
    "    def __init__(self, number_of_grids = 1, offsets = np.array([[0.0, 0.0]])) -> None:       \n",
    "        # Low value for each dimension, for each grid / tile\n",
    "        lows = np.array([[Boundary.NORTH, Boundary.WEST]]*number_of_grids)\n",
    "        # High value for each dimension, for each grid / tile\n",
    "        highs = np.array([[Boundary.SOUTH, Boundary.EAST]]*number_of_grids)\n",
    "        # Number of discrete bin for each each dimension, for each grid / tile\n",
    "        bins = np.array([[9, 9]]*number_of_grids)\n",
    "        # The offset is used to setup the overlap of grids\n",
    "        # offsets = np.array([[0.0, 0.0]]) is one grid starting from lows to highs \n",
    "        # offsets = np.array([[20.0, 20.0]]) is one grid starting from lows+[20.0, 20.0] to highs+[20.0, 20.0]\n",
    "\n",
    "        self.grids = []\n",
    "\n",
    "        for l, h, b, o in zip(lows, highs, bins, offsets):\n",
    "            grid = {}\n",
    "            grid['size']  = b\n",
    "            grid['low'] = l\n",
    "            grid['offset'] = o\n",
    "            grid['points'] = []\n",
    "            grid['step'] = []\n",
    "            for dim in range(len(b)):\n",
    "                points, step = np.linspace(l[dim], h[dim], b[dim]+1, endpoint=False, retstep=True)\n",
    "                points += o[dim]\n",
    "                grid['points'].append(points)\n",
    "                grid['step'].append(step)\n",
    "\n",
    "            grid['step'] = np.array(grid['step'])\n",
    "            grid['weights'] = np.zeros(grid['size'])\n",
    "            self.grids.append(grid)\n",
    "\n",
    "    # Get the sum of the weights for given continuous coordinates\n",
    "    def get_weight(self, sample):\n",
    "        encoded_sample = self.tile_encode(sample)\n",
    "        w = 0.0\n",
    "        for grid, (x,y) in zip(self.grids, encoded_sample):\n",
    "            w += grid['weights'][x,y]\n",
    "        return w\n",
    "\n",
    "    # Set the weights for given continuous coordinates\n",
    "    def set_weight(self, sample, target):\n",
    "        encoded_sample = self.tile_encode(sample)\n",
    "        for grid, (x,y) in zip(self.grids, encoded_sample):\n",
    "            grid['weights'][x,y] = target/len(self.grids)\n",
    "\n",
    "    # Return the discrete coordinates from continuous coordinates for all grids\n",
    "    def tile_encode(self, sample):\n",
    "        encoded_sample = []\n",
    "        for grid in self.grids:\n",
    "            encoded_sample.append(self.discretize(sample, grid))\n",
    "        return encoded_sample    \n",
    "    \n",
    "    # Return the discrete coordinates from continuous coordinates for a given grid\n",
    "    def discretize(self, sample, grid):\n",
    "        sample = np.array(sample) - (grid['low'] + grid['offset'])\n",
    "        sample = np.maximum(sample, np.array([0]*len(grid['size'])))\n",
    "        index = sample // grid['step']\n",
    "        index = np.minimum(index, grid['size']-1)\n",
    "        \n",
    "        return list(index.astype(int))\n",
    "\n",
    "    # Plot the different grids\n",
    "    def visualize_tilings(self):\n",
    "        \"\"\"Plot each tiling as a grid.\"\"\"\n",
    "        prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "        colors = prop_cycle.by_key()['color']\n",
    "        linestyles = ['-', '--', ':']\n",
    "        legend_lines = []\n",
    "        \n",
    "        for i, grid in enumerate(self.grids):\n",
    "            for x in grid['points'][0]:\n",
    "                l = plt.axvline(x=x, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)], label=i)\n",
    "            for y in grid['points'][1]:\n",
    "                l = plt.axhline(y=y, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)])\n",
    "            legend_lines.append(l)\n",
    "        plt.legend(legend_lines, [\"Tiling #{}\".format(t) for t in range(len(legend_lines))], facecolor='white', framealpha=0.9)\n",
    "        plt.title(\"Tilings\")\n",
    "\n",
    "\n",
    "# default setting for 3 tilings:\n",
    "offsets = np.array([[0.0, 0.0], [20.0, 20.0], [-20.0, 15.0]])\n",
    "tilings = grid_tilings(3, offsets)\n",
    "\n",
    "#TEST more tilings\n",
    "# offsets = np.array([ [0.0, 0.0], [20.0, 20.0], [-20.0, 15.0],[10.0, -10.0], [-10.0, -20.0],\n",
    "#    [15.0,  10.0], [-15.0, -10.0], [25.0, -25.0], [-25.0, 25.0], [5.0,   5.0] ])\n",
    "#tilings = grid_tilings(number_of_grids=10, offsets=offsets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example for one tiling\n",
    "# tilings = grid_tilings()\n",
    "\n",
    "plt.imshow(IMAGE[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "tilings.visualize_tilings()\n",
    "\n",
    "# Test with some sample values\n",
    "samples = np.random.rand(5, 2) * 264\n",
    "print(\"\\nSamples:\", samples, sep=\"\\n\")\n",
    "encoded_samples = [tilings.tile_encode(sample) for sample in samples]\n",
    "print(\"\\nIndexes of samples:\", *[s for s in encoded_samples], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following functions complete the definition of the environment. The agent's movements always go in the intended direction, but the distance travelled has a small random component. Besides by reaching the goal, the episode also terminates after TIME_LIMIT (200) steps; at that point, the agent gets a negative reward TIMEOUT_REWARD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gymnasium.Env):\n",
    "    metadata = {'render.modes': ['human', 'rgba_array']}\n",
    "    bx = np.array([AugmentedArea.WEST, AugmentedArea.WEST, AugmentedArea.EAST, AugmentedArea.EAST, AugmentedArea.WEST])\n",
    "    by = np.array([AugmentedArea.NORTH, AugmentedArea.SOUTH, AugmentedArea.SOUTH, AugmentedArea.NORTH, AugmentedArea.NORTH])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_actions = Actions\n",
    "        self.action_space = spaces.Discrete(len(self.num_actions))\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        self.display = None\n",
    "        self.img, self.img_container = Environment._init_visual_area(IMAGE)\n",
    "        self.time = 0\n",
    "\n",
    "    def seed(self, seed=None) -> int:\n",
    "        np.random.seed(seed)\n",
    "        return seed\n",
    "\n",
    "    def step(self, action: Actions):\n",
    "        assert self.action_space.contains(action)\n",
    "        (x, y), was_invalid = self._validate_state(self._move(self.state, action))\n",
    "\n",
    "        self.state = (x, y)\n",
    "        reward = MOVE_REWARD if not was_invalid else INVALID_MOVE_REWARD\n",
    "        reward = TIMEOUT_REWARD if self.time >= TIME_LIMIT else reward\n",
    "        reward = self.time * reward\n",
    "        done = is_goal_reached(self.state) or self.time >= TIME_LIMIT\n",
    "        self.time += 1\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self, state: State = None) -> State:\n",
    "        self.state = self.starting_state() if not state else state\n",
    "        self.time = 0\n",
    "        return self.state\n",
    "\n",
    "    # returns the current environment situation\n",
    "    def render(self, mode='rgba_array'):\n",
    "        curr_img = np.array(self.img_container.get_array())\n",
    "        x, y = self.state\n",
    "        scaler = 4\n",
    "        w, e, n, s = x - scaler, x + scaler, y - scaler, y + scaler\n",
    "        curr_img[n:s, w:e, 0] = 255\n",
    "        curr_img[n:s, w:e, 1] = 0\n",
    "        curr_img[n:s, w:e, 2] = 255\n",
    "        curr_img[n:s, w:e, 3] = 255\n",
    "        cropped_img = curr_img  # Just for debugging purposes\n",
    "        if mode == 'rgba_array':\n",
    "            plt.close()\n",
    "            return cropped_img  # return RGB frame suitable for video\n",
    "        elif mode == 'human':\n",
    "            container = plt.imshow(curr_img)\n",
    "            ax = container.axes\n",
    "            ax.set_xlim(Boundary.WEST, Boundary.EAST, auto=None)\n",
    "            ax.set_ylim(Boundary.SOUTH, Boundary.NORTH, auto=None)\n",
    "            return container\n",
    "        else:\n",
    "            raise Exception(f\"Please specify either 'rgba_array' or 'human' as mode parameter!\")\n",
    "\n",
    "    # Return a randomly chosen non-terminal state as starting state\n",
    "    def starting_state(self) -> State:\n",
    "        while True:\n",
    "            state = (\n",
    "                np.random.randint(Boundary.WEST, Boundary.EAST + 1),\n",
    "                np.random.randint(Boundary.NORTH, Boundary.SOUTH + 1),\n",
    "            )\n",
    "            if not is_goal_reached(state):\n",
    "                return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _move(state: State, action: Actions) -> State:\n",
    "        x, y = state\n",
    "        if action == Actions.NORTH:\n",
    "            y -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.SOUTH:\n",
    "            y += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.WEST:\n",
    "            x -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.EAST:\n",
    "            x += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_state(state: State) -> Tuple[State, bool]:\n",
    "        x, y = state\n",
    "        is_invalid = False\n",
    "        if y < Boundary.NORTH:\n",
    "            is_invalid = True\n",
    "            y = int(Boundary.NORTH)\n",
    "        if y > Boundary.SOUTH:\n",
    "            is_invalid = True\n",
    "            y = int(Boundary.SOUTH)\n",
    "        if x < Boundary.WEST:\n",
    "            is_invalid = True\n",
    "            x = int(Boundary.WEST)\n",
    "        if x > Boundary.EAST:\n",
    "            is_invalid = True\n",
    "            x = int(Boundary.EAST)\n",
    "        return (x, y), is_invalid\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_visual_area(img) -> np.ndarray:\n",
    "        x, y = img.shape\n",
    "        my_dpi = 80\n",
    "        fig = Figure(figsize=(y / my_dpi, x / my_dpi), dpi=my_dpi)\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        ax.plot(GOAL[0], GOAL[1], 'ro', linewidth=5)\n",
    "        ax.plot(Environment.bx, Environment.by, 'b-')\n",
    "        img_container = ax.imshow(img[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "        fig.tight_layout()\n",
    "        canvas.draw()  # draw the canvas, cache the renderer\n",
    "        s, (width, height) = canvas.print_to_buffer()\n",
    "        image = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n",
    "        img_container.set_data(image)\n",
    "        plt.close()\n",
    "        return image, img_container\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement your agent\n",
    "\n",
    "Next comes your part. The following class is responsible for the agent's behavior. The select_action function should implement the epsilon-greedy policy, and return an action chosen according to that policy. **Please fill in the missing codes in select_action function (1.5 points).** \n",
    "\n",
    " Remark: This is an abstract class.\n",
    " Hence, its sole purpose is creating subclasses from it, which is also the reason it cannot be instantiated.\n",
    " The following subsequent subclasses will provide a specific implementation for the methods that are missing here.\n",
    " Therefore, you can ignore the functions that are not implemented. This is just a common way to make sure that all subclasses behave similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        # set up the value of epsilon\n",
    "        self.alpha = alpha  # learning rate or step size\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.hist = []\n",
    "\n",
    "    # Choose action at state based on epsilon-greedy policy and valueFunction\n",
    "    def select_action(self, state: State, use_greedy_strategy: bool = True) -> int:\n",
    "\n",
    "        # TO BE FILLED (1.5 points)\n",
    "        # get the values \n",
    "        estimated_action_values = self.values(state)\n",
    "\n",
    "        if (np.random.rand() < self.epsilon) and not use_greedy_strategy:\n",
    "            action = np.random.randint(len(estimated_action_values)) # explore\n",
    "        else:\n",
    "            action = np.argmax(estimated_action_values) # exploit\n",
    "\n",
    "                  \n",
    "        self.hist.append(estimated_action_values)\n",
    "        return action\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    @abstractmethod\n",
    "    def value(self, state: State, action: Actions) -> float:\n",
    "        pass\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    @abstractmethod\n",
    "    def values(self, state: State) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    # Set value for given state and action\n",
    "    @abstractmethod\n",
    "    def set_value(self, state: State, action: Actions):\n",
    "        pass\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy\n",
    "    @abstractmethod\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False) -> int:\n",
    "        return None\n",
    "\n",
    "    # Return estimated state value, based on the estimated action values\n",
    "    def state_value(self, state):\n",
    "        return np.max(self.values(state))\n",
    "\n",
    "    # Plot the state value estimates. Use a larger stride for lower resolution.\n",
    "    def plot_state_values(self, stride=1):\n",
    "        self.v = np.zeros(\n",
    "            ((Boundary.SOUTH - Boundary.NORTH + stride) // stride, (Boundary.EAST - Boundary.WEST + stride) // stride))\n",
    "        for j, x in enumerate(range(Boundary.WEST, Boundary.EAST + 1, stride)):\n",
    "            for i, y in enumerate(range(Boundary.NORTH, Boundary.SOUTH + 1, stride)):\n",
    "                self.v[i, j] = self.state_value((x, y))\n",
    "\n",
    "        plt.imshow(self.v)\n",
    "        plt.colorbar()\n",
    "        return plt.show()\n",
    "\n",
    "    def plot_q_values(self, skip=1):\n",
    "        return pd.DataFrame(self.hist[::skip]).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next classes are two agents using either episodic semi-gradient Q-learning and episodic semi-gradient SARSA algorithm to estimate the value function. Both agents use the same linear function approximation method with tile coding. **Implement the `learn` function according to the update rule for the respective algorithm (1 point for each)**. \n",
    " \n",
    " REMARK: Both agents use the same tile coding. This method helps splitting the state-space into discrete chunks. Each chunk is associated with one weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles the function approximation, with several methods to query and update it. \n",
    "# A linear approximation function is used, making the computation much faster.\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "        # Use a tile coding\n",
    "        self.tilings = [grid_tilings(number_of_grids=3) for a in Actions]\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "\n",
    "    # Set value for given state and action\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        self.tilings[action].set_weight(state, value)\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy: for Qlearning, the agent does not need to return the next selected action\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False):\n",
    "        # TO BE FILLED (1 point)\n",
    "        # get the current estimated action value\n",
    "        estimated_action_value = self.value(state, action)\n",
    "        \n",
    "        if done: # compute TD error without including next state's value\n",
    "            temporal_difference = reward - estimated_action_value \n",
    "        else: # compute TD error by including the next state's value\n",
    "            temporal_difference = reward + self.gamma * np.max(self.values(next_state)) - estimated_action_value\n",
    "        \n",
    "        # update q-value\n",
    "        new_value = estimated_action_value + self.alpha * temporal_difference\n",
    "        self.set_value(state, action, new_value)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "\n",
    "        # Use a tile coding\n",
    "        self.tilings = [grid_tilings(number_of_grids=3) for a in Actions]\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "\n",
    "    # Set value for given state and action\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        self.tilings[action].set_weight(state, value)\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy: for SARSA, the agent needs to return the next selected action\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False):\n",
    "        # TO BE FILLED (1 point)\n",
    "        #raise NotImplementedError\n",
    "        current_value = self.value(state, action)\n",
    "\n",
    "        if done:\n",
    "            # terminal state\n",
    "            target = reward\n",
    "            next_action = None\n",
    "        else:\n",
    "            # Select next action (on-policy)\n",
    "            next_action = self.select_action(next_state, use_greedy_strategy=False)\n",
    "            target = reward + self.gamma * self.value(next_state, next_action)\n",
    "\n",
    "        # Update Q-value table for SARSA\n",
    "        new_value = current_value + self.alpha * (target - current_value)\n",
    "        self.set_value(state, action, new_value)\n",
    "\n",
    "        return next_action\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following function handles the interaction between agent and environment for a single episode. By passing the same value_function object in multiple calls to this function, the agent can learn from a sequence of such interactions.\n",
    "\n",
    " **Please fill in the missing parts (1 point).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: Environment in which the agent is supposed to run \n",
    "# agent: agent to learn\n",
    "# initital state: Starting state for the environment\n",
    "# is_learning: should the value function be updated during the simulation?\n",
    "# is_rendering: should the run be recorded? (Takes some time to compute)\n",
    "\n",
    "\n",
    "def run_episode(env: Environment,\n",
    "                agent: Agent,\n",
    "                initial_state: State,\n",
    "                is_learning: bool = True,\n",
    "                is_rendering: bool = False) -> Tuple[State, float]:\n",
    "    # Initialize reward for episode\n",
    "    total_reward = 0.0\n",
    "    # Initialize the policy (if is_greedy=True then the agent follows its optimal policy, otherwise it will randomly select an action)\n",
    "    is_greedy = not is_learning\n",
    "    # Get initial action\n",
    "    current_state = initial_state\n",
    "    current_action = agent.select_action(initial_state, use_greedy_strategy=is_greedy)\n",
    "\n",
    "    # Track the rendering\n",
    "    animation_data = []\n",
    "    animation_data.append((env.render(), env.time, None, current_state, None, 0))\n",
    "    # Initialize variables\n",
    "    next_state = None\n",
    "    \n",
    "    \n",
    "    done = False\n",
    "    next_action = None\n",
    "    while not done:\n",
    "        #next_state, reward, done, _ = ???\n",
    "        next_state, reward, done, _ = env.step(current_action)\n",
    "\n",
    "\n",
    "        total_reward += reward\n",
    "        if is_rendering:\n",
    "            curr_img = env.render()\n",
    "            animation_data.append((curr_img, env.time, current_state, next_state, current_action, total_reward))\n",
    "        \n",
    "        # Execute the learning and update the state and action\n",
    "        # TO BE FILLED (1 point)\n",
    "        \n",
    "        if is_learning:\n",
    "            next_action = agent.learn(current_state, current_action, next_state, reward, done)\n",
    "        else:\n",
    "            next_action = None\n",
    "            \n",
    "            \n",
    "        if next_action == None:\n",
    "            next_action = agent.select_action(next_state, use_greedy_strategy=is_greedy)\n",
    "\n",
    "        if not done:            \n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "            \n",
    "    agent.last_action = None\n",
    "    return current_state, total_reward, animation_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To help understand your agent, you can render the agent's performance by setting render to True and running the `run_epoch` function. \n",
    " \n",
    " There are some helper functions. They might help you implement the agent correctly. \n",
    " * `agent.plot_state_values` shows you how the agent values different states\n",
    " * `agent.plot_q_values` shows the q_values that the agent had over the course of his life time. (That could be a lot. There's a skip parameter to reduce the amount of data points)\n",
    "\n",
    " REMARK: Keep in mind, the following is just one example run. Don't expect the model to be fully trained after just one episode. The training part follows in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "epsilon = 0.0\n",
    "gamma = .7\n",
    "alpha = 1e-2\n",
    "agent = QAgent(alpha, gamma, epsilon)\n",
    "start_state = env.reset()\n",
    "end_state, total_reward, animation_data = run_episode(env, agent, start_state, is_learning=True, is_rendering=True)\n",
    "agent.plot_state_values()\n",
    "agent.plot_q_values(skip=1)\n",
    "render_epoch(animation_data, interval=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the simulation, play with parameters and analyse results\n",
    "\n",
    " Now it's time to train both algorithms/agents on the environment. \n",
    " \n",
    " In the simulations, please plot the measure of each algorithm's performance as a function of the episode (i.e. the sum of all immediate rewards received in each episode). You shall play with a few combinations of two parameters discounted factor $\\gamma$ and stepsize $\\alpha$ (at least two variables for each parameter). During the experiments, keep $\\epsilon$ fixed at $0.01$. A reasonable starting point for $\\alpha$ is 1e-2. \n",
    " \n",
    " REMARK: You can save the parameters and update-to-date Q values of each agent, so that you can still test their performance later. (You can achieve this by keeping the object.)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Please submit your code as well as the plots presenting compariable performance of the different combinations of parameters for every algorithms (2 points)**. \n",
    "\n",
    "REMARK: For a decent comparison all agents should be plotted on the same axis bounds. Also, the plots may be hard to interpret because of the scales. Feel free to do your own smoothing or use the `smooth` function provided in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for running the simulation and ploting the sum of immediate reward in each episode.\n",
    "def exponential_smooth(data, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Exponential smoothing for a given list of data.\n",
    "    alpha: smoothing factor (0 < alpha <= 1)\n",
    "    \"\"\"\n",
    "    smoothed = []\n",
    "    s = data[0]  \n",
    "    for x in data:\n",
    "        s = alpha * x + (1 - alpha) * s\n",
    "        smoothed.append(s)\n",
    "    return smoothed\n",
    "\n",
    "# Hyper Parameters\n",
    "num_episodes = 500\n",
    "gamma = 0.5 # discount factor  ( how much we value future rewards ) \n",
    "alpha = 1e-2     # learning rate \n",
    "epsilon = 1e-1  # exploration rate\n",
    "# Init the environment and run Q-Learning & SARSA\n",
    "env = Environment()\n",
    "q_agent   =       QAgent(alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "sarsa_agent = SARSAAgent(alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "#experiments \n",
    "# Run Q-learning\n",
    "q_rewards = []\n",
    "for i in range(num_episodes):\n",
    "    start_state = env.reset()\n",
    "    _, total_reward, _ = run_episode(env, q_agent, start_state, is_learning=True, is_rendering=False)\n",
    "    q_rewards.append(total_reward)\n",
    "\n",
    "# Run SARSA\n",
    "sarsa_rewards = []\n",
    "for i in range(num_episodes):\n",
    "    start_state = env.reset()\n",
    "    _, total_reward, _ = run_episode(env, sarsa_agent, start_state, is_learning=True, is_rendering=False)\n",
    "    sarsa_rewards.append(total_reward)\n",
    "\n",
    "# Save and plot the results\n",
    "\n",
    "# smoothing to make it readable ! ! ! ( better then last time)\n",
    "q_smoothed = exponential_smooth(q_rewards, alpha=0.05)\n",
    "sarsa_smoothed = exponential_smooth(sarsa_rewards, alpha=0.05)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(q_rewards, alpha=0.3, label='Q-Learning ', color=(0.0, 0.0, 0.9)) # blue\n",
    "plt.plot(q_smoothed, label='Q-Learning (smoothed)', color=(0.1, 0.1, 1.0))\n",
    "plt.plot(sarsa_rewards, alpha=0.3, label='SARSA (raw)', color=(0.9,0.3,0.3  )  ) # pink\n",
    "plt.plot(sarsa_smoothed, label='SARSA (smoothed)', color=(1.0, 0.1, 0.1))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning vs SARSA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# TO BE FILLED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_start_state = env.reset()\n",
    "\n",
    "_, demo_total_reward, demo_animation_data = run_episode(env,q_agent, demo_start_state, is_learning=False, is_rendering=True )\n",
    "\n",
    "print(f\"Total reward in demo run q_agent : {demo_total_reward}\")\n",
    "\n",
    "# Render the demo episode as an animation\n",
    "render_epoch(demo_animation_data, interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_start_state = env.reset()\n",
    "\n",
    "_, demo_total_reward, demo_animation_data = run_episode(env,sarsa_agent, demo_start_state, is_learning=False, is_rendering=True )\n",
    "\n",
    "print(f\"Total reward in demo run sarsa_agent: {demo_total_reward}\")\n",
    "\n",
    "# Render the demo episode as an animation\n",
    "render_epoch(demo_animation_data, interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your plots here. (In case your code takes too long to run.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Please interpret the results/plots (2 points)**. Explain how two algorithms differ in the learning procedure from another and what the results say about the parameters alpha (step-size) and gamma (decay-rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Let us think one step further by looking at the policies you have learned (1.5 points).** \n",
    "\n",
    "Please compare the performance of your learned optimal policies using another simulation. In this simulation, you shall have three agents (Qlearning_Agent, SARSA_agent and Random_agent), where all three agents always use their initial policy to behave and find the goal in the given map (make sure the agent does NOT learn anymore!). The initial policies of Qlearning_Agent and SARSA_agent are the optimal policies learned from the above experiments (based on the final estimated Q-values using Q-learning and SARSA algorithm respectively). The inital policy of Random_Agent will select 4 actions randomly. \n",
    "\n",
    "Describe the performance of three agents by running each agent on the task and discuss the results (1.5 points). You can use the render function provided above (and other helper functions) observe the different behaviors of three agents. You could also use an appropriate plot to show the different performances. *There is no need to submit your codes for this question.*\n",
    "\n",
    "Some questions you shall think about and answer (in case there are no differences or no special things, just indicate what you observed):\n",
    "- How does the learned Q-learning policy and SARSA policy perform differently? Does it show the difference between on-policy and off-policy methods?\n",
    "- What kind of strategy did the two RL agents learn similarly? How do they differ from the random policy?\n",
    "- For the RL-learned policies, do you still observe something that does not perform so well? Can you think about possible ways to improve/solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost done! Before handing in, make sure that the code you hand in work, and that all plots are shown. **Submit just one file per team.**\n",
    "\n",
    "Again, make sure **you name this file according to your last names**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
